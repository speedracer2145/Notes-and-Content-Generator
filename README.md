# üìÑ PDF Query System with LangChain, ChromaDB, and Local LLM

This project is a **PDF Query System** that lets you interact with uploaded PDFs by asking questions and receiving context-driven answers. It uses:

- **LangChain** for retrieval-augmented generation (RAG)  
- **ChromaDB** for vector storage and retrieval  
- **Sentence Transformers** for local embeddings  
- **Ollama** for running a local LLM (privacy-first, offline)  
- **Streamlit** for a clean, interactive frontend  

---

## üöÄ Features

- Query PDFs in natural language  
- Retrieval-Augmented Generation with **LangChain**  
- Store and retrieve embeddings with **ChromaDB**  
- Generate embeddings using **Sentence Transformers** (`all-MiniLM-L6-v2`)  
- Run a local **LLM via Ollama** (no external API keys needed)  
- Streamlit-based interactive web interface  

---

## üìÇ Project Structure

app.py # Main Streamlit application
requirements.txt # Required Python packages
README.md # Project documentation
/content/chroma_db/ # Persistent ChromaDB storage
sample_pdfs/ # Example PDFs for testing

---

## ‚öôÔ∏è Installation & Setup

### 1. Prerequisites
- Python **3.8+**  
- [Streamlit](https://streamlit.io/)  
- [Ollama](https://ollama.ai/) installed and running locally  
- Required Python packages (`requirements.txt`)  

### 2. Create and Activate Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```
### Running the Ollama Server Locally

The application is configured to run completely locally without the need for external API keys. Ensure that the Ollama server is up and running on `http://127.0.0.1:11434`.

```bash
# Check if Ollama is running on the correct port
lsof -i :11434

# Start server (if needed)
# Note: Port conflicts may require using a different port.
```

### Running the Application

1. Clone the repository and navigate to the project folder:

    ```bash
    git clone <repository_url>
    cd <project_folder>
    ```

2. Add your PDF files to the `pdf_paths` list in `app.py`.

3. Run the Streamlit application:

    ```bash
    streamlit run app.py
    ```

4. Access the application at `http://localhost:8501`.

## Usage

1. Start the application and ensure your PDFs are uploaded.
2. Enter a question related to the PDF content.
3. The system retrieves relevant chunks from ChromaDB and provides a response generated by the local LLM.
4. Sources of the response are displayed for reference.

## Code Highlights

- **Custom LLM Integration**: The `CustomOllamaLLM` class in `app.py` is designed to interface with Ollama's local server, ensuring all queries are processed locally without external API keys.
- **LangChain and ChromaDB**: Using LangChain with ChromaDB facilitates efficient retrieval and response generation, enabling accurate document querying.
- **Local Embedding Generation**: Sentence Transformers run locally to convert document content into embeddings, stored in ChromaDB for quick access.

## Known Issues

- **Port Conflicts**: Ensure no multiple instances of Ollama are running. Use `lsof` to troubleshoot conflicts.
- **Server Unavailability**: If the Ollama server is unreachable, confirm it‚Äôs installed and running on the expected port.

## Future Improvements

- **Real-time PDF Uploads**: Add Streamlit-based upload functionality for a more dynamic interface.
- **Advanced LLMs**: Experiment with alternative LLMs for enhanced conversational capabilities.
- **Expanded Retrieval Options**: Explore additional retrieval methods within LangChain to improve response accuracy.

## Requirements

- Python 3.8+
- `fitz` for PDF handling
- `langchain` for LLM-based document processing
- `streamlit` for UI development
- `chromadb` for vector storage and retrieval
